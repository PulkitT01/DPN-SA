Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
############### Propensity Score SAE net Training ###############
.. Training started ..
##### train e2e #########
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.035499667150922766
Epoch: 200, loss: 0.021685956252945796
Epoch: 300, loss: 0.019670788037740153
Epoch: 400, loss: 0.019487700206630022
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.747243769466877, correct: 2333/2570, accuracy: 0.9077821011673152
Epoch: 50, loss: 14.409273196011782, correct: 2335/2570, accuracy: 0.9085603112840467
########## train layer wise all layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08746315695253419
Epoch: 200, loss: 0.08585634330908458
Epoch: 300, loss: 0.08060912346398388
Epoch: 400, loss: 0.08194199052673799
----- Training SAE -----
Epoch: 100, loss: 0.08736517271141947
Epoch: 200, loss: 0.08792251274909502
Epoch: 300, loss: 0.08757754139326236
Epoch: 400, loss: 0.08700743855701552
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.533512510359287, correct: 2330/2570, accuracy: 0.9066147859922179
Epoch: 50, loss: 14.958076305687428, correct: 2340/2570, accuracy: 0.9105058365758755
########## train layer wise only newly stacked layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08715753698790515
Epoch: 200, loss: 0.08093617759920933
Epoch: 300, loss: 0.08517377290092869
Epoch: 400, loss: 0.08060922974973549
----- Training SAE -----
Epoch: 100, loss: 0.0916507289181521
Epoch: 200, loss: 0.08870677475208118
Epoch: 300, loss: 0.08512312514178547
Epoch: 400, loss: 0.09210126709055018
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.945780705660582, correct: 2334/2570, accuracy: 0.9081712062256809
Epoch: 50, loss: 15.246849585324526, correct: 2328/2570, accuracy: 0.9058365758754864
Training completed..
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
treated: 237
control: 2333
total: 2570
[0.04051900655031204, 0.025978298857808113, 0.0019974426832050085, 0.08065955340862274, 0.39522647857666016, 0.07536730170249939, 0.0857234001159668, 0.02901044860482216, 0.3623299300670624, 0.07503087818622589, 0.8704609870910645, 0.946525514125824, 0.13055938482284546, 0.3851684033870697, 0.31974828243255615, 0.39010247588157654, 0.3967154920101166, 0.3968067765235901, 0.8363710641860962, 0.3917457163333893, 0.9227580428123474, 0.3791942894458771, 0.4878668785095215, 0.24614883959293365, 0.3837088644504547, 0.09412223845720291, 0.9774083495140076, 0.3660115897655487, 0.3906484544277191, 0.19599370658397675, 0.38081833720207214, 0.3857596218585968, 0.3460107743740082, 0.3971456289291382, 0.3923240900039673, 0.1130361333489418, 0.3902847468852997, 0.3962836265563965, 0.35996800661087036, 0.0917012020945549, 0.3880978226661682, 0.9795005321502686, 0.3949485123157501, 0.395559698343277, 0.27669185400009155, 0.39623770117759705, 0.38446044921875, 0.3920306861400604, 0.3980260491371155, 0.2949540615081787, 0.36569783091545105, 0.9728982448577881, 0.22604157030582428, 0.9793517589569092, 0.3871402442455292, 0.030631523579359055, 0.3955745995044708, 0.8699237108230591, 0.3678945302963257, 0.38371482491493225, 0.39325761795043945, 0.3957262337207794, 0.39283329248428345, 0.39551815390586853, 0.39684993028640747, 0.3774462938308716, 0.3726118505001068, 0.39711853861808777, 0.39124366641044617, 0.39650097489356995, 0.3977794647216797, 0.39724743366241455, 0.35931405425071716, 0.39078372716903687, 0.3961277902126312, 0.39694854617118835, 0.3849068880081177, 0.3978117108345032, 0.3971286416053772, 0.3911367654800415, 0.39717844128608704, 0.3970259130001068, 0.3970297873020172, 0.39684075117111206, 0.3949052691459656, 0.3703266680240631, 0.3971088230609894, 0.3971088230609894, 0.3423202931880951, 0.3970228433609009, 0.3968123495578766, 0.39687713980674744, 0.33755728602409363, 0.2978673577308655, 0.39588168263435364, 0.37685808539390564, 0.39590463042259216, 0.3977094292640686, 0.39701417088508606, 0.39255571365356445, 0.3777763843536377, 0.39794403314590454, 0.7791523337364197, 0.3812451958656311, 0.3103305399417877, 0.39532050490379333, 0.3977801203727722, 0.3959024250507355, 0.39194455742836, 0.3993441164493561, 0.5099924206733704, 0.38213053345680237, 0.3774462938308716, 0.39302390813827515, 0.9829877614974976, 0.3957628607749939, 0.3966229557991028, 0.3936865031719208, 0.4000311493873596, 0.4288721978664398, 0.3963771164417267, 0.9736142754554749, 0.396781325340271, 0.3977094292640686, 0.3977094292640686, 0.39841213822364807, 0.24371279776096344, 0.5100578665733337, 0.3962831199169159, 0.34413984417915344, 0.3975716233253479, 0.3975716233253479, 0.3961889147758484, 0.39711499214172363, 0.39748939871788025, 0.39384526014328003, 0.39740049839019775, 0.39740049839019775, 0.39574602246284485, 0.39769604802131653, 0.3781338632106781, 0.39194050431251526, 0.6823292970657349, 0.39698681235313416, 0.3965766429901123, 0.3968915641307831, 0.39565348625183105, 0.3963499367237091, 0.5323089361190796, 0.397051066160202, 0.3868674337863922, 0.3972405195236206, 0.3959512412548065, 0.101118303835392, 0.3965051472187042, 0.40518665313720703, 0.8593524694442749, 0.39724743366241455, 0.3975256681442261, 0.39784276485443115, 0.3978389799594879, 0.39771077036857605, 0.3975455164909363, 0.3949137330055237, 0.39692315459251404, 0.39724743366241455, 0.3970043659210205, 0.39628446102142334, 0.3968915641307831, 0.3968915641307831, 0.397203266620636, 0.3968033790588379, 0.39739716053009033, 0.39706793427467346, 0.8610442876815796, 0.39674896001815796, 0.3973698616027832, 0.39731869101524353, 0.35852864384651184, 0.3975253701210022, 0.3974229693412781, 0.3974229693412781, 0.39159244298934937, 0.3975808620452881, 0.396954208612442, 0.3976151943206787, 0.39721426367759705, 0.3972373902797699, 0.3978117108345032, 0.3975256681442261, 0.3975256681442261, 0.3971197009086609, 0.39711904525756836, 0.39770781993865967, 0.39716437458992004, 0.3972220718860626, 0.39723193645477295, 0.3967375159263611, 0.39708083868026733, 0.39679762721061707, 0.39668333530426025, 0.3970394730567932, 0.39765435457229614, 0.39761456847190857, 0.39714372158050537, 0.3970179557800293, 0.3970191776752472, 0.3955434560775757, 0.3971506655216217, 0.3971700668334961, 0.3971644937992096, 0.39762550592422485, 0.3976905941963196, 0.397080659866333, 0.39711764454841614, 0.3971295654773712, 0.3970743715763092, 0.3970828950405121, 0.3971088230609894, 0.39746201038360596, 0.39739683270454407, 0.3976483941078186, 0.39654257893562317, 0.39716124534606934, 0.39687153697013855, 0.39719340205192566, 0.3972541093826294, 0.3973793387413025, 0.3973793387413025, 0.8552179336547852, 0.3968658447265625, 0.39665818214416504, 0.3960970640182495, 0.39608848094940186, 0.41469964385032654, 0.4083411991596222, 0.39557433128356934]
Using original data
cuda:0
{'epochs': 400, 'lr': 0.001, 'batch_size': 32, 'shuffle': True, 'sparsity_probability': 0.8, 'weight_decay': 0.0003, 'BETA': 0.1}
########### 400 epochs ###########
----------------------------------------
iter_id: 0
----------------------------------------
Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
Numpy Test Statistics:
torch.Size([642, 19])
torch.Size([642, 1])
----------- Training and evaluation phase ------------
############### Propensity Score neural net Training ###############
.. Training started ..
Saved model path: ./Propensity_Model/NN_PS_model_iter_id_0_epoch_50_lr_0.001.pth
Epoch: 25, loss: 15.318436194211245, correct: 2332/2570, accuracy: 0.9073929961089494
Epoch: 50, loss: 14.830190550535917, correct: 2336/2570, accuracy: 0.9089494163424124
Saved model..
.. Propensity score evaluation started using NN..
.. Propensity score evaluation completed using NN..
############### DCN Training using NN ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
Saved model path: ./DCNModel/NN_DCN_model_iter_id_0_epoch_100_lr_0.0001.pth
.. Training started ..
cuda:0
epoch: 9, Treated + Control loss: 860.8199898546966
epoch: 19, Treated + Control loss: 804.6008783054535
epoch: 29, Treated + Control loss: 812.457100531509
epoch: 39, Treated + Control loss: 782.5793255730176
epoch: 49, Treated + Control loss: 799.7189083839364
epoch: 59, Treated + Control loss: 772.3228268698238
epoch: 69, Treated + Control loss: 757.9986842567487
epoch: 79, Treated + Control loss: 753.8105039828263
epoch: 89, Treated + Control loss: 762.4967477599087
epoch: 99, Treated + Control loss: 763.1432310712529
Neural Net start time: = 2024-12-21 23:07:23.141124
Neural Net end time: = 2024-12-21 23:11:58.001738
{'epochs': 400, 'lr': 0.001, 'batch_size': 32, 'shuffle': True, 'train_set': <torch.utils.data.dataset.TensorDataset object at 0x154643336b90>, 'sparsity_probability': 0.8, 'weight_decay': 0.0003, 'BETA': 0.1, 'input_nodes': 17}
############### Propensity Score SAE net Training ###############
.. Training started ..
##### train e2e #########
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.035261654375511924
Epoch: 200, loss: 0.021907911107036066
Epoch: 300, loss: 0.02053137895087769
Epoch: 400, loss: 0.019310203725817026
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 14.87615429982543, correct: 2341/2570, accuracy: 0.9108949416342412
Epoch: 50, loss: 14.278306890279055, correct: 2346/2570, accuracy: 0.91284046692607
########## train layer wise all layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.0835788126712964
Epoch: 200, loss: 0.08379548843260165
Epoch: 300, loss: 0.08301259729045408
Epoch: 400, loss: 0.08099945712788606
----- Training SAE -----
Epoch: 100, loss: 0.09410539674170224
Epoch: 200, loss: 0.09068567480570004
Epoch: 300, loss: 0.09351807924700373
Epoch: 400, loss: 0.0859499574167493
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.978458859026432, correct: 2326/2570, accuracy: 0.9050583657587549
Epoch: 50, loss: 14.771962236613035, correct: 2342/2570, accuracy: 0.911284046692607
########## train layer wise only newly stacked layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.09049202253421147
Epoch: 200, loss: 0.08779108459934776
Epoch: 300, loss: 0.07975239805693243
Epoch: 400, loss: 0.08136848976582657
----- Training SAE -----
Epoch: 100, loss: 0.08928292157289422
Epoch: 200, loss: 0.09154186917491901
Epoch: 300, loss: 0.09197280712333726
Epoch: 400, loss: 0.09408825562324052
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.617346432060003, correct: 2334/2570, accuracy: 0.9081712062256809
Epoch: 50, loss: 15.004177406430244, correct: 2334/2570, accuracy: 0.9081712062256809
Training completed..
---------------------------------------------------------------------------
End to End SAE training
---------------------------------------------------------------------------
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
############### DCN Training using SAE ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
Saved model path: ./DCNModel/SAE_E2E_DCN_model_iter_id_0_epoch_100_lr_0.0001.pth
.. Training started ..
cuda:0
epoch: 9, Treated + Control loss: 831.159440960917
epoch: 19, Treated + Control loss: 843.3708096504993
epoch: 29, Treated + Control loss: 804.297397792327
epoch: 39, Treated + Control loss: 766.9644937529854
epoch: 49, Treated + Control loss: 787.7640911746759
epoch: 59, Treated + Control loss: 780.6760482199032
epoch: 69, Treated + Control loss: 751.5367880247886
epoch: 79, Treated + Control loss: 758.8189978156327
epoch: 89, Treated + Control loss: 769.7531864083593
epoch: 99, Treated + Control loss: 775.1047375669503
SAE E2E start time: = 2024-12-21 23:16:59.747662
SAE E2E end time: = 2024-12-21 23:21:28.825330
---------------------------------------------------------------------------
----------Layer wise greedy stacked SAE training - All layers----------
---------------------------------------------------------------------------
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
############### DCN Training using SAE ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
Saved model path: ./DCNModel/SAE_stacked_all_DCN_model_iter_id_0_epoch_100_lr_0.0001.pth
.. Training started ..
cuda:0
epoch: 9, Treated + Control loss: 845.9258624509989
epoch: 19, Treated + Control loss: 809.8068493572747
epoch: 29, Treated + Control loss: 768.097288536394
epoch: 39, Treated + Control loss: 804.4485832894136
epoch: 49, Treated + Control loss: 786.630697968744
epoch: 59, Treated + Control loss: 766.6480047618011
epoch: 69, Treated + Control loss: 745.3436657926577
epoch: 79, Treated + Control loss: 755.6440452389345
epoch: 89, Treated + Control loss: 745.9124563223289
epoch: 99, Treated + Control loss: 752.2080588631475
SAE all layer active start time: = 2024-12-21 23:21:28.825359
SAE all layer active end time: = 2024-12-21 23:25:58.930490
---------------------------------------------------------------------------
Layer wise greedy stacked SAE training - Current layers
---------------------------------------------------------------------------
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
############### DCN Training using SAE ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
Saved model path: ./DCNModel/SAE_stacked_cur_DCN_model_iter_id_0_epoch_100_lr_0.0001.pth
.. Training started ..
cuda:0
epoch: 9, Treated + Control loss: 845.1021390808796
epoch: 19, Treated + Control loss: 832.2709317272993
epoch: 29, Treated + Control loss: 792.758546940551
epoch: 39, Treated + Control loss: 795.1822685235456
epoch: 49, Treated + Control loss: 773.6281996350307
epoch: 59, Treated + Control loss: 775.4156230157704
epoch: 69, Treated + Control loss: 769.689578605599
epoch: 79, Treated + Control loss: 760.2201386554148
epoch: 89, Treated + Control loss: 762.2930900682175
epoch: 99, Treated + Control loss: 731.5668726106874
SAE cur layer active start time: = 2024-12-21 23:25:58.930517
SAE all layer active end time: = 2024-12-21 23:30:29.520342
