Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
############### Propensity Score SAE net Training ###############
.. Training started ..
##### train e2e #########
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.029989725530699448
Epoch: 200, loss: 0.02088198024365637
Epoch: 300, loss: 0.019111081210459457
Epoch: 400, loss: 0.018434252003552736
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.36567248031497, correct: 2329/2570, accuracy: 0.9062256809338521
Epoch: 50, loss: 14.184264570474625, correct: 2340/2570, accuracy: 0.9105058365758755
########## train layer wise all layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.0848426168071635
Epoch: 200, loss: 0.08493149648478002
Epoch: 300, loss: 0.08335948197378053
Epoch: 400, loss: 0.08299195849233204
----- Training SAE -----
Epoch: 100, loss: 0.09262497911666646
Epoch: 200, loss: 0.10555777376816597
Epoch: 300, loss: 0.09033553201107332
Epoch: 400, loss: 0.08484454636956439
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.492953918874264, correct: 2333/2570, accuracy: 0.9077821011673152
Epoch: 50, loss: 15.086614288389683, correct: 2333/2570, accuracy: 0.9077821011673152
########## train layer wise only newly stacked layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08570457953545782
Epoch: 200, loss: 0.08247963927778197
Epoch: 300, loss: 0.08229210200133147
Epoch: 400, loss: 0.08343857131254526
----- Training SAE -----
Epoch: 100, loss: 0.08888834323964001
Epoch: 200, loss: 0.08937396394249833
Epoch: 300, loss: 0.088985822358985
Epoch: 400, loss: 0.09094253366376147
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 16.057249628007412, correct: 2331/2570, accuracy: 0.9070038910505837
Epoch: 50, loss: 15.245916806161404, correct: 2335/2570, accuracy: 0.9085603112840467
Training completed..
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
treated: 237
control: 2333
total: 2570
[0.043276384472846985, 0.019502541050314903, 0.016343209892511368, 0.09498406946659088, 0.3536105155944824, 0.08344544470310211, 0.11743782460689545, 0.035832591354846954, 0.34799888730049133, 0.1798885613679886, 0.9346078038215637, 0.8994634747505188, 0.05906445533037186, 0.03859928622841835, 0.038247428834438324, 0.28479358553886414, 0.44776833057403564, 0.806609034538269, 0.95184725522995, 0.4436856210231781, 0.7809077501296997, 0.30864280462265015, 0.22346824407577515, 0.19048821926116943, 0.5160443782806396, 0.20004849135875702, 0.9202930331230164, 0.3537433445453644, 0.4432675838470459, 0.09576604515314102, 0.28490889072418213, 0.4852306842803955, 0.4523578882217407, 0.45034462213516235, 0.5148753523826599, 0.33882468938827515, 0.46881285309791565, 0.5423506498336792, 0.545293390750885, 0.2291044294834137, 0.4403643012046814, 0.9720529317855835, 0.527865469455719, 0.5209558010101318, 0.29199275374412537, 0.6724841594696045, 0.44120660424232483, 0.4490451216697693, 0.4417542517185211, 0.24162402749061584, 0.46927541494369507, 0.9633134603500366, 0.10849407315254211, 0.9627490639686584, 0.4401029646396637, 0.02291151136159897, 0.7905897498130798, 0.44746842980384827, 0.3931461572647095, 0.44021546840667725, 0.46101582050323486, 0.44799700379371643, 0.4197779893875122, 0.448371022939682, 0.4512074887752533, 0.4307728409767151, 0.4373663067817688, 0.44817399978637695, 0.4436701238155365, 0.45684048533439636, 0.46039772033691406, 0.4480458199977875, 0.4752415120601654, 0.4164433181285858, 0.5351966619491577, 0.4547790586948395, 0.4416537582874298, 0.44770631194114685, 0.4503467082977295, 0.42729389667510986, 0.4510972499847412, 0.4559687077999115, 0.4554772973060608, 0.4510675072669983, 0.5551567077636719, 0.41375619173049927, 0.44772273302078247, 0.44772273302078247, 0.23964163661003113, 0.454243004322052, 0.45472198724746704, 0.4543716609477997, 0.2624632716178894, 0.22029587626457214, 0.45131534337997437, 0.42687538266181946, 0.5341814160346985, 0.46616342663764954, 0.44726768136024475, 0.502165675163269, 0.43669581413269043, 0.44726991653442383, 0.4446677267551422, 0.4363449215888977, 0.2000555843114853, 0.4516220688819885, 0.4674500524997711, 0.44748297333717346, 0.46940097212791443, 0.47039079666137695, 0.30835866928100586, 0.4616628587245941, 0.4307728409767151, 0.5219528079032898, 0.8389587998390198, 0.4466087520122528, 0.5567737817764282, 0.4452824890613556, 0.4306624233722687, 0.47810596227645874, 0.5544489622116089, 0.8161171674728394, 0.5519148707389832, 0.46616342663764954, 0.46616342663764954, 0.4437865614891052, 0.3495829999446869, 0.29941487312316895, 0.5430530905723572, 0.43902355432510376, 0.4630928635597229, 0.4630928635597229, 0.533954918384552, 0.4566885828971863, 0.4614406228065491, 0.5008097290992737, 0.45987531542778015, 0.45987531542778015, 0.5314105153083801, 0.4591969847679138, 0.38175129890441895, 0.43785011768341064, 0.4263838529586792, 0.44890454411506653, 0.44857078790664673, 0.4480806887149811, 0.44796910881996155, 0.4480694830417633, 0.31284886598587036, 0.4505803883075714, 0.45450735092163086, 0.4484684467315674, 0.44801539182662964, 0.2189566045999527, 0.455984890460968, 0.41092735528945923, 0.4365789294242859, 0.4480458199977875, 0.4475962817668915, 0.4477347731590271, 0.44777530431747437, 0.44776642322540283, 0.44781455397605896, 0.45884254574775696, 0.4519536793231964, 0.4480458199977875, 0.4558934271335602, 0.4524431824684143, 0.4480806887149811, 0.4480806887149811, 0.44802501797676086, 0.4594442546367645, 0.44839054346084595, 0.44806957244873047, 0.44743049144744873, 0.4582458734512329, 0.4479822814464569, 0.4479230046272278, 0.2722238600254059, 0.44828498363494873, 0.44800642132759094, 0.44800642132759094, 0.4080369770526886, 0.4479522705078125, 0.4509817659854889, 0.44782447814941406, 0.45075762271881104, 0.4509327709674835, 0.44770631194114685, 0.4475962817668915, 0.4475962817668915, 0.4540599286556244, 0.4540998339653015, 0.44788968563079834, 0.4505554139614105, 0.4507974684238434, 0.4508158564567566, 0.44871535897254944, 0.45069611072540283, 0.4496998190879822, 0.4486778974533081, 0.4543454349040985, 0.4480457305908203, 0.44760486483573914, 0.4539443552494049, 0.4552062153816223, 0.4554082453250885, 0.44318556785583496, 0.4544861912727356, 0.45464351773262024, 0.45400774478912354, 0.447827011346817, 0.44761860370635986, 0.452974408864975, 0.45358866453170776, 0.4559429883956909, 0.4555044174194336, 0.45558077096939087, 0.44772273302078247, 0.44890087842941284, 0.44760116934776306, 0.4476470649242401, 0.4516771137714386, 0.45604923367500305, 0.4560646414756775, 0.4477294385433197, 0.4516042172908783, 0.4537161886692047, 0.4537161886692047, 0.44758841395378113, 0.44743406772613525, 0.4473804831504822, 0.44772276282310486, 0.44765323400497437, 0.4474165141582489, 0.44703274965286255, 0.4486904442310333]
Using original data
cuda:0
{'epochs': 400, 'lr': 0.001, 'batch_size': 32, 'shuffle': True, 'sparsity_probability': 0.8, 'weight_decay': 0.0003, 'BETA': 0.1}
########### 400 epochs ###########
----------------------------------------
iter_id: 0
----------------------------------------
Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
Numpy Test Statistics:
torch.Size([642, 19])
torch.Size([642, 1])
----------- Training and evaluation phase ------------
############### Propensity Score neural net Training ###############
.. Training started ..
Saved model path: ./Propensity_Model/NN_PS_model_iter_id_0_epoch_50_lr_0.001.pth
Epoch: 25, loss: 15.334063984453678, correct: 2333/2570, accuracy: 0.9077821011673152
Epoch: 50, loss: 14.508892487734556, correct: 2335/2570, accuracy: 0.9085603112840467
Saved model..
.. Propensity score evaluation started using NN..
.. Propensity score evaluation completed using NN..
############### DCN Training using NN ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
