Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
############### Propensity Score SAE net Training ###############
.. Training started ..
##### train e2e #########
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.03549643146035112
Epoch: 200, loss: 0.023340524772159112
Epoch: 300, loss: 0.019981767062419727
Epoch: 400, loss: 0.019252613899700434
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.551208011806011, correct: 2336/2570, accuracy: 0.9089494163424124
Epoch: 50, loss: 14.742017440497875, correct: 2351/2570, accuracy: 0.9147859922178988
########## train layer wise all layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08994225035479039
Epoch: 200, loss: 0.08503406062538241
Epoch: 300, loss: 0.07879235411499753
Epoch: 400, loss: 0.08144808781367761
----- Training SAE -----
Epoch: 100, loss: 0.08843508509942043
Epoch: 200, loss: 0.08377319399589374
Epoch: 300, loss: 0.08949778297985042
Epoch: 400, loss: 0.08552935544723346
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.853781387209892, correct: 2322/2570, accuracy: 0.9035019455252918
Epoch: 50, loss: 15.023026440292597, correct: 2341/2570, accuracy: 0.9108949416342412
########## train layer wise only newly stacked layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08560066582796014
Epoch: 200, loss: 0.08427724035249816
Epoch: 300, loss: 0.07958535716673475
Epoch: 400, loss: 0.07733384809560245
----- Training SAE -----
Epoch: 100, loss: 0.09216888529466993
Epoch: 200, loss: 0.08740102550313797
Epoch: 300, loss: 0.09067118770362419
Epoch: 400, loss: 0.08618721853435775
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.861640572547913, correct: 2329/2570, accuracy: 0.9062256809338521
Epoch: 50, loss: 15.246518209576607, correct: 2334/2570, accuracy: 0.9081712062256809
Training completed..
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
treated: 237
control: 2333
total: 2570
[0.10596872866153717, 0.02098679170012474, 0.004094812087714672, 0.04572148248553276, 0.3900940418243408, 0.11081543564796448, 0.13619746267795563, 0.020957371219992638, 0.15210753679275513, 0.052915848791599274, 0.9380424618721008, 0.9706783294677734, 0.09323335438966751, 0.05669209361076355, 0.04517343267798424, 0.27692312002182007, 0.39022955298423767, 0.3748464286327362, 0.6391059160232544, 0.388997346162796, 0.9065464735031128, 0.35631510615348816, 0.4246751368045807, 0.19205845892429352, 0.5095775723457336, 0.21810735762119293, 0.9902366995811462, 0.3192293345928192, 0.3879258334636688, 0.08796310424804688, 0.36348438262939453, 0.5444710850715637, 0.37117913365364075, 0.4022120237350464, 0.45615988969802856, 0.5477206110954285, 0.3834230601787567, 0.5314318537712097, 0.4042353928089142, 0.08080262690782547, 0.3366243839263916, 0.9909811615943909, 0.5443003177642822, 0.5256896018981934, 0.3353888690471649, 0.3982962667942047, 0.3553256094455719, 0.3023894727230072, 0.5469751358032227, 0.2641960382461548, 0.38486501574516296, 0.975075900554657, 0.08748968690633774, 0.9884627461433411, 0.3620746433734894, 0.0386619046330452, 0.40938013792037964, 0.3970142900943756, 0.3466707766056061, 0.36175140738487244, 0.3437693119049072, 0.39472922682762146, 0.46750184893608093, 0.3879127502441406, 0.40369704365730286, 0.3723878562450409, 0.34290120005607605, 0.39853549003601074, 0.369546115398407, 0.39874517917633057, 0.5272600054740906, 0.3921949863433838, 0.3845033049583435, 0.3856739401817322, 0.553826093673706, 0.41023772954940796, 0.345797061920166, 0.39245644211769104, 0.40571895241737366, 0.40886759757995605, 0.4119430184364319, 0.4179769456386566, 0.41644349694252014, 0.4064514636993408, 0.5633186101913452, 0.38965338468551636, 0.42362749576568604, 0.42362749576568604, 0.22977851331233978, 0.40916696190834045, 0.4154530167579651, 0.4119279384613037, 0.23005838692188263, 0.15191318094730377, 0.36161375045776367, 0.2825509011745453, 0.5443588495254517, 0.5022911429405212, 0.45473095774650574, 0.38989248871803284, 0.3447565734386444, 0.42413046956062317, 0.4563106596469879, 0.3652874529361725, 0.3064313530921936, 0.35949164628982544, 0.49670547246932983, 0.3534766137599945, 0.3903947174549103, 0.46411457657814026, 0.6289724707603455, 0.3758249878883362, 0.3723878562450409, 0.5338786840438843, 0.9664615392684937, 0.37407881021499634, 0.5474327206611633, 0.39755979180336, 0.3280980587005615, 0.519828200340271, 0.543953001499176, 0.9145622253417969, 0.5611879825592041, 0.5022911429405212, 0.5022911429405212, 0.35205817222595215, 0.09198855608701706, 0.6401475071907043, 0.5575385689735413, 0.36512893438339233, 0.511110782623291, 0.511110782623291, 0.5525967478752136, 0.521905779838562, 0.5145247578620911, 0.516223132610321, 0.5173459053039551, 0.5173459053039551, 0.5509034991264343, 0.5618544220924377, 0.16133227944374084, 0.3892950415611267, 0.32581162452697754, 0.5036245584487915, 0.3898431360721588, 0.39248281717300415, 0.39159467816352844, 0.3928013741970062, 0.837001383304596, 0.43924570083618164, 0.395290732383728, 0.3947080075740814, 0.3927515745162964, 0.1732434183359146, 0.39496469497680664, 0.32766979932785034, 0.4840199649333954, 0.3921949863433838, 0.3983011543750763, 0.39220839738845825, 0.3920421004295349, 0.3947816491127014, 0.42531755566596985, 0.3873833417892456, 0.41477781534194946, 0.3921949863433838, 0.4631747603416443, 0.39290302991867065, 0.39248281717300415, 0.39248281717300415, 0.3943748474121094, 0.43063414096832275, 0.3960883915424347, 0.39233681559562683, 0.38715609908103943, 0.4107072353363037, 0.3949533700942993, 0.3940229117870331, 0.31889814138412476, 0.39757877588272095, 0.3920716941356659, 0.3920716941356659, 0.4252394437789917, 0.3919813334941864, 0.4016442596912384, 0.39445939660072327, 0.40859663486480713, 0.40960752964019775, 0.39245644211769104, 0.3983011543750763, 0.3983011543750763, 0.4145103394985199, 0.414337694644928, 0.39193788170814514, 0.4067561626434326, 0.40646106004714966, 0.4060462713241577, 0.41173475980758667, 0.4049341678619385, 0.40801772475242615, 0.4095195531845093, 0.40808942914009094, 0.40092891454696655, 0.3975859582424164, 0.41316163539886475, 0.41543498635292053, 0.4158124327659607, 0.42064422369003296, 0.4078413248062134, 0.4079829156398773, 0.4120904803276062, 0.4252643287181854, 0.3968964219093323, 0.412576824426651, 0.4181100130081177, 0.4182470142841339, 0.416671484708786, 0.41735202074050903, 0.42362749576568604, 0.4592893719673157, 0.4027767479419708, 0.39977699518203735, 0.4139437675476074, 0.4189682602882385, 0.4186289310455322, 0.41986319422721863, 0.4726436138153076, 0.5363538265228271, 0.5363538265228271, 0.29450684785842896, 0.39673057198524475, 0.4534977376461029, 0.44594326615333557, 0.44069868326187134, 0.22140620648860931, 0.43707790970802307, 0.42370888590812683]
Using original data
cuda:0
{'epochs': 400, 'lr': 0.001, 'batch_size': 32, 'shuffle': True, 'sparsity_probability': 0.8, 'weight_decay': 0.0003, 'BETA': 0.1}
########### 400 epochs ###########
----------------------------------------
iter_id: 0
----------------------------------------
Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
Numpy Test Statistics:
torch.Size([642, 19])
torch.Size([642, 1])
----------- Training and evaluation phase ------------
############### Propensity Score neural net Training ###############
.. Training started ..
Saved model path: ./Propensity_Model/NN_PS_model_iter_id_0_epoch_50_lr_0.001.pth
Epoch: 25, loss: 15.325193418189883, correct: 2331/2570, accuracy: 0.9070038910505837
Epoch: 50, loss: 14.360477793961763, correct: 2356/2570, accuracy: 0.9167315175097276
Saved model..
.. Propensity score evaluation started using NN..
.. Propensity score evaluation completed using NN..
############### DCN Training using NN ###############
Treated Statistics ==>
torch.Size([237, 17])
Control Statistics ==>
torch.Size([2333, 17])
