Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
############### Propensity Score SAE net Training ###############
.. Training started ..
##### train e2e #########
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.027959064546006697
Epoch: 200, loss: 0.020548525841239795
Epoch: 300, loss: 0.013961594804753492
Epoch: 400, loss: 0.01290033601316405
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 14.757520351558924, correct: 2345/2570, accuracy: 0.9124513618677043
Epoch: 50, loss: 13.970674119889736, correct: 2354/2570, accuracy: 0.9159533073929961
########## train layer wise all layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.0864372829688184
Epoch: 200, loss: 0.08627337493278363
Epoch: 300, loss: 0.08462715047745058
Epoch: 400, loss: 0.08183123277109346
----- Training SAE -----
Epoch: 100, loss: 0.09057568022866308
Epoch: 200, loss: 0.08775593000061718
Epoch: 300, loss: 0.08819060147176554
Epoch: 400, loss: 0.08941584945092966
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.787344068288803, correct: 2333/2570, accuracy: 0.9077821011673152
Epoch: 50, loss: 15.108774088323116, correct: 2342/2570, accuracy: 0.911284046692607
########## train layer wise only newly stacked layer active ############
Training mode: train
----- Training SAE -----
Epoch: 100, loss: 0.08507278837539532
Epoch: 200, loss: 0.08261169014889518
Epoch: 300, loss: 0.0847046910152759
Epoch: 400, loss: 0.08056810516634105
----- Training SAE -----
Epoch: 100, loss: 0.09148653394278185
Epoch: 200, loss: 0.08631931245326996
Epoch: 300, loss: 0.08783158347194578
Epoch: 400, loss: 0.09105220758988533
.. Propensity score evaluation started using Sparse AE..
Epoch: 25, loss: 15.662596933543682, correct: 2333/2570, accuracy: 0.9077821011673152
Epoch: 50, loss: 15.348471738398075, correct: 2329/2570, accuracy: 0.9062256809338521
Training completed..
.. Propensity score evaluation started using Sparse AE ..
.. Propensity score evaluation completed Sparse AE ..
treated: 237
control: 2333
total: 2570
[0.09051325917243958, 0.03214135393500328, 0.00397910363972187, 0.061523303389549255, 0.676400899887085, 0.04111362621188164, 0.047910742461681366, 0.05414805933833122, 0.3121474087238312, 0.12137984484434128, 0.9734554290771484, 0.9767060875892639, 0.11344058066606522, 0.040280263870954514, 0.038675349205732346, 0.3273630738258362, 0.3593119978904724, 0.7373628616333008, 0.8162493109703064, 0.4076775908470154, 0.9201998114585876, 0.33785873651504517, 0.13985785841941833, 0.14820824563503265, 0.4675227403640747, 0.12318910658359528, 0.9850000143051147, 0.40909087657928467, 0.40763965249061584, 0.061240304261446, 0.39041727781295776, 0.5210407376289368, 0.3306666314601898, 0.3770636022090912, 0.428743839263916, 0.6250892877578735, 0.33391767740249634, 0.4919586777687073, 0.4407350420951843, 0.10838049650192261, 0.33029037714004517, 0.9897045493125916, 0.3395438492298126, 0.34595316648483276, 0.2759878635406494, 0.594191312789917, 0.3704375624656677, 0.3031240999698639, 0.4355929493904114, 0.17055092751979828, 0.2856444716453552, 0.9646887183189392, 0.06807093322277069, 0.97217857837677, 0.3851560652256012, 0.044830892235040665, 0.7754277586936951, 0.34881818294525146, 0.2213371992111206, 0.37738215923309326, 0.47800230979919434, 0.37992414832115173, 0.37954747676849365, 0.3013618290424347, 0.362881600856781, 0.3735930621623993, 0.5265790224075317, 0.36290186643600464, 0.38885772228240967, 0.3852648138999939, 0.3878267705440521, 0.3571930527687073, 0.26940929889678955, 0.38637039065361023, 0.4458063542842865, 0.37232959270477295, 0.3568836748600006, 0.3576279282569885, 0.36774682998657227, 0.37230366468429565, 0.3687937557697296, 0.36021891236305237, 0.3610234260559082, 0.36579132080078125, 0.6412560939788818, 0.36643555760383606, 0.3565317392349243, 0.3565317392349243, 0.33492475748062134, 0.3668195307254791, 0.3687798082828522, 0.3695122003555298, 0.17065826058387756, 0.1384868174791336, 0.3736279010772705, 0.3508594036102295, 0.4835295081138611, 0.4033752381801605, 0.35151541233062744, 0.5904228687286377, 0.3217085003852844, 0.35058629512786865, 0.8367798328399658, 0.38519471883773804, 0.3350948095321655, 0.37146899104118347, 0.39941224455833435, 0.35111314058303833, 0.41997888684272766, 0.384321928024292, 0.684654176235199, 0.3613273501396179, 0.3735930621623993, 0.45634225010871887, 0.9602979421615601, 0.3522206246852875, 0.6865574717521667, 0.36539918184280396, 0.29798173904418945, 0.40794557332992554, 0.5086954236030579, 0.9520299434661865, 0.49827179312705994, 0.4033752381801605, 0.4033752381801605, 0.2285601645708084, 0.23006632924079895, 0.9214326739311218, 0.44425395131111145, 0.20775285363197327, 0.41153237223625183, 0.41153237223625183, 0.45955297350883484, 0.42895379662513733, 0.4156421422958374, 0.29614001512527466, 0.4196656346321106, 0.4196656346321106, 0.38874414563179016, 0.44533419609069824, 0.2192862182855606, 0.3111788332462311, 0.08906786888837814, 0.4691448211669922, 0.3871661424636841, 0.35811474919319153, 0.3672003448009491, 0.3615597188472748, 0.7483444809913635, 0.35853007435798645, 0.4610753059387207, 0.37373796105384827, 0.364393413066864, 0.052289873361587524, 0.37779197096824646, 0.2953258454799652, 0.7880359292030334, 0.3571930527687073, 0.35678622126579285, 0.35738569498062134, 0.3571874499320984, 0.35661283135414124, 0.35727590322494507, 0.41307446360588074, 0.36914554238319397, 0.3571930527687073, 0.3962094187736511, 0.38472530245780945, 0.35811474919319153, 0.35811474919319153, 0.36101657152175903, 0.4053924083709717, 0.3703099489212036, 0.35754796862602234, 0.3448694348335266, 0.3924078941345215, 0.35950610041618347, 0.35742518305778503, 0.20884445309638977, 0.3672332465648651, 0.3569994270801544, 0.3569994270801544, 0.38895556330680847, 0.35692712664604187, 0.370643675327301, 0.3567653298377991, 0.36587759852409363, 0.36545318365097046, 0.3576279282569885, 0.35678622126579285, 0.35678622126579285, 0.3620316684246063, 0.36186856031417847, 0.3569468855857849, 0.3672381639480591, 0.36547359824180603, 0.3649924397468567, 0.3755064606666565, 0.36902859807014465, 0.3758127987384796, 0.3761362135410309, 0.36755138635635376, 0.3624984622001648, 0.356675922870636, 0.3623681962490082, 0.3612511157989502, 0.3608704209327698, 0.376225084066391, 0.3640140891075134, 0.36363357305526733, 0.36259523034095764, 0.3572528660297394, 0.3565911650657654, 0.36501118540763855, 0.3667285144329071, 0.3694109320640564, 0.36124470829963684, 0.3613646626472473, 0.3565317392349243, 0.3570334315299988, 0.3562517464160919, 0.35643109679222107, 0.37512528896331787, 0.36193764209747314, 0.3641483783721924, 0.35673385858535767, 0.3584196865558624, 0.38654187321662903, 0.38654187321662903, 0.3454877734184265, 0.35364434123039246, 0.3511269688606262, 0.35242176055908203, 0.3517286479473114, 0.3460429310798645, 0.3421439826488495, 0.35192742943763733]
Using original data
cuda:0
{'epochs': 400, 'lr': 0.001, 'batch_size': 32, 'shuffle': True, 'sparsity_probability': 0.8, 'weight_decay': 0.0003, 'BETA': 0.1}
########### 400 epochs ###########
----------------------------------------
iter_id: 0
----------------------------------------
Numpy Train Statistics:
torch.Size([2570, 19])
torch.Size([2570, 1])
Numpy Test Statistics:
torch.Size([642, 19])
torch.Size([642, 1])
----------- Training and evaluation phase ------------
